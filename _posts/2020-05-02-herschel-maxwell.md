---
title: "Аксиомы Хершела-Максвелла"
layout: post
tags: [normal distribution, statistics, probability, mathematics, chi-squared]
---

Обычно нормальное распределение вводят с бухты-барахты, точнее с функции плотности. 
Примерно так. Стандартным нормальным распределением называется распределение с функцией плотности вида:
$$f(x) = \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{x^2}{2}\right)$$

И сразу у студентов возникает вопрос: откуда взялась такая функция плотности? 
На него обычно ответа не дают :) Иногда доказывают, что это корректная функция плотности, 
так как интеграл под ней равен единице.

Здесь я хочу популяризовать другой подход. А именно, аксиомы Хершела-Максвелла! 
Они однозначно задают нормальное распределение!

Сначала мотивация.

Поймаем в комнате молекулу кислорода и посмотрим на вектор её скоростей $(X_1, X_2, X_3)$.
Это случайный вектор. 

Во-первых, если в комнате нет сквозняка, то разумно считать, что нет доминирующего направления. 
Все направления равновероятны. 
Другими словами, если повернуть систему координат, то мы ожидаем, что закон распределения 
случайного вектора не изменится.

Во-вторых, если по горизонтальной составляющей молекула движется на запад, то это не говорит нам о том,
поднимается она или опускается. 

В-третьих, мы сами можем выбрать единицы измерения скорости. 

Эти три интуитивных наблюдения превращаются в строгие аксиомы Хершела-Максвелла. 


> Мы говорим, что вектор $X=(X_1, X_2, \ldots, X_n)$ имеет стандартное многомерное нормальное распределение, если выполнены три предпосылки:
>
> A1. Инвариантность к вращениям. Закон распределения вектора $X$ сохраняется при любом вращении. 
> 
> A2. Независимось ортогональных проекций. Проекции вектора $X$ на ортогональные подпространства независимы.
> 
> A3. Нормировка. Пусть $E(X_1^2)=1$.

Оказывается, эти три аксиомы однозначно говорят, что компоненты $(X_i)$ независимы и имеют стандартное нормальное распределение. 


Полное доказательство можно найти у Wlodzimierz Bryc в [Normal Distribution characterizations with applications](https://homepages.uc.edu/~brycwz/probab/charakt/charakt.pdf).

Краткое доказательство с дополнительным предположением существования функции плотности есть у Jaynes в [Probability: the logic of science](http://www-biba.inrialpes.fr/Jaynes/cc07s.pdf).


Для случая двух величин аксиомы Хершела-Максвелла можно упростить:

> B1. Величины $X_1$ и $X_2$ независимы и одинаково распределены. 
> 
> B2. Величины $X-Y$ и $X+Y$ независимы.
>
> В3. Пусть $E(X_1^2)=1$.

Прежде всего новое определение прекрасно тем, что позволяет доказывать многие теоремы практически устно, 
если разглядеть в формулировке теоремы проекции и геометрию. Например :)

> Для стандартного нормального случайного вектора $X$ величины $\bar X$ и $\hat\sigma^2$ независимы.

Доказательство:

> Заметим, что проекция вектора $X$ на прямую, порожденную вектором $(1, 1, \ldots, 1)$, это не что иное,
> как вектор $(\bar X, \bar X, \ldots, \bar X)$. А проекция вектора $X$ на ортогональное дополнение этой прямой 
> будет вектором $(X_1 - \bar X, X_2 - \bar X, \ldots, X_n - \bar X)$. Эти векторы независимы по аксиоме А2.
> Значит и любые функции от этих векторов — независимы. Остаётся заметить, что $\hat\sigma^2$ — с точностью до деления
> на $(n-1)$ равна квадрату длины проекции.

А дальше можно ввести хи-квадрат распределение с геометрическим вкусом.


## Хи-квадрат

> Если вектор $X$ имеет многомерное стандартное нормальное распределение, то мы говорим, что 
> квадрат длины проекции на любое $d$-мерное подпространство в $\mathbb{R}^n$ имеет хи-квадрат распределение с $d$ степенями свободы.

То есть степени свободы — это размерность подпространства, на которое мы проецируем стандартный нормальный вектор.
Новое определение гораздо более геометрическое и смысловое, чем классическое:

> Случайная величина $Y$ имеет хи-квадрат распределение с $n$ степенями свободы, если она представима в виде 
> $$Y = X_1^2 + X_2^2 + \ldots + X_n^2$$
> где $X_i$ имеют стандартное нормальное распределение и независимы.

Если взять в новом определении $d=n$, то новое определение превратится в классическое.
Подпространство совпадёт со всем пространством, проекция
совпадёт с исходным вектором $X$, и длина проекции окажется равна именно $X_1^2 + X_2^2 + \ldots + X_n^2$.

Строго говоря, необходимо доказывать непротиворечивость нового определения.
А вдруг закон распределения длины проекции зависит от конкретного выбора подпространства, а не только от его размерности $d$?

Корректность нового определения следует из того, что любые два $d$-мерных пространства переводятся друг в друга некоторым вращением.
А согласно аксиоме А1, при вращении закон распределения вектора $X$ не изменяется.


## Другие аксиоматические подходы


Есть куча аксиоматических способо определить нормальное распределение. 
Все они прекрасны тем, что из слов рождается сама собой функция плотности с $e$, $\pi$ и $\sqrt{.}$.
Они менее удобные, чем аксиомы Хершела-Максвелла, для дальнейшей работы, но их красота завораживает.

Почему нормальное распределение называется распределением Гаусса? Гаусс решил тысячелетний спор между астрономами,
которые до этого двумя лагерями спорили, как примирить противоречивые показания свидетелей о положении звёзд и планет. 
Оказывается нормальное распределение, выборочное среднее и метод максимального правдоподобия связаны аксиомой:

> Предположим, что  $(X_i)$ независимы, одинаково распределены с математическим ожиданием $\mu$ и дисперсией $\sigma^2$.
> Оценкой для $\mu$ методом максимального правдоподобия будет среднее арифметическое $\bar X$, если и только если $X_i \sim N(\mu, \sigma^2)$.

Гаусс вывел нормальную функцию плотности из данного определения. Подробно написано у Saul Stahl, [Evolution of the normal distribution](https://www.maa.org/sites/default/files/pdf/upload_library/22/Allendoerfer/stahl96.pdf).

Есть аксиома Штейна. Мы знаем, что для любых величин $Cov(aX + b, Y) = aCov(X, Y)$. А если хочется простого обобщения этого свойства, то оно есть только у нормальных величин:

> Если $Cov(h(X),Y) = E(h'(X))Cov(X,Y)$ для всех функций $h$, то речь идёт о двумерном нормальном распределении $(X, Y)$.


!Найти ссылку! Немного есть на [wikipedia](https://en.wikipedia.org/wiki/Stein%27s_lemma).


Есть и определение через дифференциальную энтропию:

> Случайная величина $X$ имеет математическое ожидание $\mu$ и дисперсию $\sigma^2$. 
> Дифференциальная энтропия величины $X$ будет минимальной, если и только если $X_i \sim N(0;1)$.

Доказательство у Keith Conrad, [Probability distribution and maximum entropy](https://kconrad.math.uconn.edu//blurbs/analysis/entropypost.pdf).

И через ЦПТ:

> Если в семействе живут случайные величины нулевым ожиданием и конечной дисперсией, а сумма двух независимых величин этого семейства распределений принадлежит ему же с точностью до масштабирования, то речь идёт о нормальном семействе.

Jaynes пишет, что идею стабильности при суммировании впервые [сформулировал Vernon Landon](http://www-biba.inrialpes.fr/Jaynes/cc07s.pdf), и приводит современный вариант доказательства. Имя Vernon Landon, увы, практически неизвестно. 

Давайте же рассказывать про красивую аксиоматику, из которой следует формула плотности нормального распределения :)



По мотивам поста на [CrossValidated](https://stats.stackexchange.com/questions/4364).

Cobb про [преподавание статистики](http://chjs.mat.utfsm.cl/volumes/02/01/Cobb(2011).pdf).

